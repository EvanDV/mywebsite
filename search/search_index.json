{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Research Page","text":"<p> Welcome! My name is Evan Davies-Velie, I am a first-year Master's student at McGill University as a part of the McGill Cosmology Instrumentation Laboratory led by Professor Matt Dobbs. </p> <p>My research focuses on analyzing Fast Radio Bursts (FRBs) \u2014 powerful, millisecond-long radio pulses from deep in the universe whose origins remain largely unknown. These bursts are thought to originate from highly energetic astrophysical phenomena. I am interested in how we can use FRBs as cosmological probes to learn more about the history, evolution, and composition of the universe.</p> <p>Currently, I work with CHIME (the Canadian Hydrogen Intensity Mapping Experiment), a groundbreaking radio telescope located at the NRC\u2019s Dominion Radio Astrophysical Observatory (DRAO), located just south of Penticton, BC. Originally designed to map large-scale cosmic structures, CHIME emerged as a world leader in FRB science. In addition, I am involved in commissioning CHORD (the Canadian Hydrogen Observatory and Radio-transient Detector), an upcoming radio telescope set to extend CHIME's capabilities with higher sensitivity, resolution, and broader frequency coverage. Together, these instruments offer unprecedented opportunities to study FRBs, map the large-scale structure of the universe, and deepen our understanding of the cosmos.</p> <p>Thank you for visiting, and feel free to explore more about my journey in radio astronomy!</p> <p>If you're like to learn more about me, you can find an introduction to my research, some info on some recent projects and some more info about me.</p> <p> CHIME is a novel radio telescope that has no moving parts. Originally conceived to map the most abundant element in the universe - hydrogen - over a good fraction of the observable universe, this unusual telescope is optimized to have a high \"mapping speed\", which requires a large instantaneous field of view (~200 square degrees) and broad frequency coverage (400-800 MHz). The digitized signals collected by CHIME will be processed to form a 3-dimensional map of hydrogen density, which will be used to measure the expansion history of the universe. At the same time, these signals can be combed for fast, transient radio emission, making CHIME a unique telescope for discovering new \"Fast Radio Bursts\" and for monitoring many pulsars on a daily basis.</p>"},{"location":"about/","title":"Introduction","text":"<p>My name is Evan Davies-Velie, I am a radio astronomer focussed on both data analysis and instrumentation. </p> <p>I completed my B.Sc. in Joint Honours Physics and Computer Science at McGill University. During my undergraduate studies I had the opportunity to work with data from the Canadian Hydrogen Intensity Mapping Experiment (CHIME). You can find more about my projects involing CHIME here.</p> <p>I have also contributed to the Canadian Hydrogen Observatory and Radio-transient Detector (CHORD) by analyzing system temperatures of the Deep Dish Development Array (D3A). To analyze data from this prototype telescope, I had to work with extremely low level data. This hands-on experience has deepened my understanding of radio telescope instrumentation and data handling, which is crucial for both future instrumentation projects and data analysis.</p> <p>These experiences have solidified my commitment to advancing research in radio astronomy, particularly in the context of cutting-edge Canadian instruments like CHIME and CHORD. I am excited to continue my journey in this field, contributing to the development and science operations of these pioneering telescopes.</p> <p>Through my TAing experiences during my undergraduate and as a graduate student I have found a passion for teaching. I would like to go on to earn my PhD and eventually become a professor. As a graduate student, I am looking forward to further developing my teaching and communication skills.</p> <p>Beyond physics I am very interested in music. I love searching for niche songs and undiscovered artists. I compile these in playlists that you can view on my Spotify. I plan to volunteer at the CKUT and hopefully get my own show eventually! </p>"},{"location":"about/#cv","title":"CV","text":""},{"location":"about/contact/","title":"Contact Information","text":"<p>Email: evan.davies-velie@mail.mcgill.ca</p> <p>Office: Rutherford Room 232</p>"},{"location":"projects/","title":"Projects","text":"<p>Here are some projects that I am working on. </p> <ul> <li> <p>Blazar Variability Analysis Using WWZ is an amazing python project that will allow you to see my coding skills.</p> <ul> <li>CHIME Meeting Presentation</li> </ul> </li> <li> <p>Outreach Project</p> </li> </ul>"},{"location":"projects/presentation/","title":"Presentation","text":""},{"location":"projects/python_project/WWZ/","title":"Detectings QPOs Using Weighted Wavelet Z-Transform (WWZ)","text":"In\u00a0[2]: Copied! <pre>import emcee\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\nfrom tqdm import tqdm\nfrom scipy.optimize import minimize\nimport scipy.signal as signal\nimport sys\nfrom hampel import hampel\nimport pandas as pd \nimport scipy.linalg\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport time\nimport numexpr as ne\nimport multiprocessing as mp\nfrom collections import defaultdict\nmp.set_start_method('fork')\n</pre> import emcee import numpy as np import matplotlib.pyplot as plt import csv from tqdm import tqdm from scipy.optimize import minimize import scipy.signal as signal import sys from hampel import hampel import pandas as pd  import scipy.linalg from mpl_toolkits.axes_grid1 import make_axes_locatable import time import numexpr as ne import multiprocessing as mp from collections import defaultdict mp.set_start_method('fork') <p>Wavelet transforms perform a similar function to the Fourier transform, but, in addition to this, they can break signals down into oscillations localized in space and time. However, when applied to unevenly sampled time series, the response of the wavelet transform is often more dependent on irregularities in the number and spacing of available data than on actual changes in the parameters of the signal. Yet by casting the wavelet transform as a projection, we can derive its statistical behavior and devise advantageous rescaled transforms. By treating it as a weighted projection to form the weighted wavelet Z-transform (WWZ), we improve its ability to detect, and especially to quantify, periodic and pseudo-periodic signals.</p> <p>We perform a weighted projection onto the three trial functions</p> <p>$$\\phi_1(t) = \\textbf{1}(t)$$</p> <p>$$\\phi_2(t) = \\cos(\\omega (t-\\tau))$$</p> <p>$$\\phi_3(t) = \\sin(\\omega (t-\\tau))$$</p> <p>Where we compute the weighted projection by defining the inner product of two function $f(t)$ and $g(t)$ as \\begin{equation} \\langle f|g \\rangle = \\frac{\\sum_{\\alpha=1}^N w_\\alpha f(t_\\alpha )g(t_\\alpha ) }{\\sum_{\\beta=1}^N w_\\beta} \\end{equation}</p> <p>and with the statistical weights modified to include the inverse variance weights from CHIME, the statistical weight assigned to a data point $\\alpha$ is defined as \\begin{equation} w_{\\alpha} = \\frac{1}{\\sigma_\\alpha^2} \\cdot e^{-c\\omega^2(t_\\alpha - \\tau)^2} \\end{equation}</p> <p>Notice the factor of $\\omega$ in the exponential, the size of the \u201cwindow\u201d is frequency-dependent. The constant $c$ determines how rapidly the window decays. We have chosen $c$ such that 3 periods can fit within the full width at half maximum (FWHM) of the Gaussian window:</p> <p>$$ e^{-c\\omega^2(t-\\tau)^2} = e^{\\frac{-(t-\\tau)^2}{2\\sigma^2}}$$ $$c\\omega^2 = \\frac{1}{2\\sigma^2} \\implies c \\left(\\frac{2\\pi}{T}\\right)^2 = \\frac{1}{2\\sigma^2}$$ $$c = \\frac{T^2}{8\\pi^2\\sigma^2}$$ If we want the FWHM to contain 3 periods we must have that $2.355\\sigma = 3T$. Therefore $\\sigma = \\frac{3T}{2.355}$ and Plugging this in we get $$c = \\frac{1}{8\\left(\\frac{3}{2.355}\\right)^2\\pi^2} \\approx 0.0078$$</p> <p>Projection computes the coefficients $y_a$ of a set of r trial functions for which the model function: $$y(t) = \\sum_a y_a \\phi_a(t),$$ best fits the data (in the sense that it minimizes the sum of the squared residuals).</p> <p>We then compute the S-matrix, which is the matrix of the inner products of the trial functions \\begin{equation} S_{ab} = \\langle \\phi_a|\\phi_b \\rangle \\end{equation}</p> <p>We determine the best-fit coefficients of our trial functions by multiplying the inverse of the S-matrix by the vector of inner products of the trial functions with the data</p> <p>\\begin{equation} y_a = \\sum_{b} S_{ab}^{-1} \\langle \\phi_b|x \\rangle \\end{equation}</p> <p>We define the power (which we use to evaluate the projection statistically) by \\begin{equation} P = \\frac{N}{(r-1)s^2} \\left(\\sum_{a,b}S^{-1}_{ab}\\langle \\phi_a|x \\rangle \\langle \\phi_b|x \\rangle - \\langle \\textbf{1}|x \\rangle^2 \\right) \\end{equation}</p> <p>where $N$ is the number, and $s^2$ the estimated variance of the data.</p> <p>A modification imposed by a weighted projection is that in the definition of the power we must replace the number of data points N by the effective number:</p> <p>\\begin{equation} N_{\\text{eff}} = \\frac{(\\Sigma w_\\alpha)^2}{(\\Sigma w_\\alpha^2)} \\end{equation}</p> <p>and instead use the weighted estimated variance \\begin{equation} s_w^2 = \\frac{N_{\\text{eff}}V_x}{N_{\\text{eff}}-1}  \\end{equation}</p> <p>where $V_x$ is the weighted variation of the data</p> <p>\\begin{equation} V_x = \\frac{\\Sigma_\\alpha w_\\alpha x^2(t_\\alpha)}{\\Sigma_\\lambda w_\\lambda} - \\left[\\frac{\\Sigma_\\alpha w_\\alpha x(t_\\alpha)}{\\Sigma_\\lambda w_\\lambda}\\right]^2 = \\langle x|x\\rangle - \\langle \\textbf{1}|x \\rangle^2 \\end{equation}</p> <p>and similarly, the weighted variation of the model function is</p> <p>\\begin{equation} V_y = \\frac{\\Sigma_\\alpha w_\\alpha y^2(t_\\alpha)}{\\Sigma_\\lambda w_\\lambda} - \\left[\\frac{\\Sigma_\\alpha w_\\alpha y(t_\\alpha)}{\\Sigma_\\lambda w_\\lambda}\\right]^2 = \\langle y|y\\rangle - \\langle \\textbf{1}|y \\rangle^2 \\end{equation}</p> <p>Foster then goes on to define the weighted wavelet transform (WWT) as \\begin{equation} WWT = \\frac{ (N_{\\text{eff}}-1)V_y }{ 2V_x } \\end{equation}</p> <p>For fixed parameters $\\omega$ and $\\tau$, the WWT may be treated as a chi-square statistic with two degrees of freedom and expected value 1.</p> <p>The above expected values for the WWT only hold when the data is random noise. For a sinusoidal signal, one might expect the WWT to peak at the signal frequency, but instead it tends to peak at a lower frequency, due to an effect which strongly impacts the WWT. At lower frequencies, our \u201cwindow\u201d is wider, so we effectively sample more data points; the effective number $N_{\\text{eff}}$ is larger. This can make the WWT increase with decreasing $\\omega$, even though the fit to the data is poorer, simply because the factor $N_{\\text{eff}}$ is larger.</p> <p>We would get a better estimate of the frequency of a significant peak if we had a test statistic which was less sensitive to the effective number of data. Fortunately, there is such a statistic for projections: Foster applies the Z-statistic of Foster (1996a) which he dubs the weighted wavelet Z-transform (WWZ):</p> <p>\\begin{equation} Z = \\frac{ (N_{\\text{eff}}-3)V_y }{ 2(V_x-V_y) } \\end{equation}</p> In\u00a0[3]: Copied! <pre># https://articles.adsabs.harvard.edu//full/1996AJ....112.1709F/0001709.000.html\n\n# Eq. 5.3, but with optional noise weights included\ndef wa(omega,tau,t,c=0.0125,weights=None):\n    wavelet = np.exp(-1*c*(omega[:,np.newaxis,np.newaxis]\n                           *(t[np.newaxis,np.newaxis,:]-tau[np.newaxis,:,np.newaxis]))**2)\n    if weights is not None:\n        wavelet *= weights\n    return wavelet\n\n# Eq. 5.4\ndef Neff(omega,tau,t,c=0.0125,weights=None):\n    weight = wa(omega,tau,t,c=c,weights=weights)\n    return np.square(np.sum(weight,axis=-1)) / np.sum(np.square(weight),axis=-1)\n\n# Eq. 5.5-5.7\ndef phi(n,omega,tau,t):\n    if n==0:\n        return np.ones((len(omega),len(tau),len(t)))\n    if n==1:\n        return np.cos(omega[:,np.newaxis,np.newaxis]\n                      *(t[np.newaxis,np.newaxis,:]-tau[np.newaxis,:,np.newaxis]))\n    if n==2:\n        return np.sin(omega[:,np.newaxis,np.newaxis]\n                      *(t[np.newaxis,np.newaxis,:]-tau[np.newaxis,:,np.newaxis]))\n    \ndef make_phi(omega,tau,t):\n    phi0 = np.ones((len(omega),len(tau),len(t)))\n    phi1 = np.cos(omega[:,np.newaxis,np.newaxis]*(t[np.newaxis,np.newaxis,:]-tau[np.newaxis,:,np.newaxis]))\n    phi2 = np.sin(omega[:,np.newaxis,np.newaxis]*(t[np.newaxis,np.newaxis,:]-tau[np.newaxis,:,np.newaxis]))\n    return np.array([phi0,phi1,phi2])\n\n# Eq. 4.2\ndef inner(f,g,omega,tau,t,weight,c=0.0125,weights=None):\n    tensor = ne.evaluate('weight*f*g')\n    return np.sum(tensor,axis=-1) / np.sum(weight,axis=-1)\n\n    \n# Eq. 4.3\ndef S_inv(omega,tau,t,weight,phi_array,c=0.0125,weights=None):\n    Sij = np.ones((len(omega),len(tau),3,3))\n    for i in range(3):\n        for j in range(3):\n            Sij[...,i,j] = inner(phi_array[i],phi_array[j],omega,tau,t,weight,c=c,weights=weights)\n    Sinv = np.linalg.inv(Sij)\n    return Sinv\n\n\ndef y(x,omega,tau,t,Sinv,weight,phi_array,c=0.0125,weights=None):\n    # returns matrix with shape (omega,tau,t)\n    yij = np.ones((len(omega),len(tau),3,3))\n    for i in range(3):\n        for j in range(3):\n            yij[...,i,j] = Sinv[...,i,j] * inner(phi_array[j],x,omega,tau,t,weight,c=c,weights=weights)\n    # Eq. 4.4\n    yi = np.sum(yij,axis=-1)\n    # Eq. 4.1\n    yt = np.zeros((len(omega),len(tau),len(t)))\n    for i in range(3):\n        yt += yi[...,i][...,np.newaxis] * phi_array[i]\n    return yt\n\n# Eq. 5.9/10\ndef weighted_variation(x,omega,tau,t,weight,c=0.0125,weights=None):\n    # returns matrix with shape (omega, tau)\n    xx = inner(x,x,omega,tau,t,weight,c=c,weights=weights)\n    x1 = inner(1,x,omega,tau,t,weight,c=c,weights=weights)\n    #return xx-x1**2\n    return ne.evaluate('xx-x1**2')\n        \n# Eq. 5.12\ndef Z(x,omega,tau,t,Sinv,N,weight,phi_array,c=0.0125,weights=None):\n    Vx = weighted_variation(x,omega,tau,t,weight,c=c,weights=weights)\n    Vy = weighted_variation(y(x,omega,tau,t,Sinv,weight,phi_array,c=c,weights=weights),omega,tau,t,weight,c=c,weights=weights)\n    N[N&lt;3] = 3 #Stops WWZ from going negative for some sources\n    return ne.evaluate('(N-3) * Vy / (2 * (Vx-Vy))')\n\n#Eq. 5.14\ndef A(x,omega,tau,t,c=0.0125,weights=None):\n    # returns matrix with shape (omega,tau)\n    yij = np.ones((len(omega),len(tau),3,3))\n    weight = wa(omega,tau,t,c=c,weights=weights)\n    Sinv = np.linalg.inv(S(omega,tau,t,c=c,weights=weights))\n    for i in range(3):\n        for j in range(3):\n            yij[...,i,j] = Sinv[...,i,j] * inner(phi(j,omega,tau,t),x,omega,tau,t,c=c,weights=weights)\n    # Eq. 4.4\n    yi = np.sum(yij,axis=-1)\n    y2 = yi[...,1][...]\n    y3 = yi[...,2][...]\n    return np.sqrt(np.square(y2)+np.square(y3))\n</pre> # https://articles.adsabs.harvard.edu//full/1996AJ....112.1709F/0001709.000.html  # Eq. 5.3, but with optional noise weights included def wa(omega,tau,t,c=0.0125,weights=None):     wavelet = np.exp(-1*c*(omega[:,np.newaxis,np.newaxis]                            *(t[np.newaxis,np.newaxis,:]-tau[np.newaxis,:,np.newaxis]))**2)     if weights is not None:         wavelet *= weights     return wavelet  # Eq. 5.4 def Neff(omega,tau,t,c=0.0125,weights=None):     weight = wa(omega,tau,t,c=c,weights=weights)     return np.square(np.sum(weight,axis=-1)) / np.sum(np.square(weight),axis=-1)  # Eq. 5.5-5.7 def phi(n,omega,tau,t):     if n==0:         return np.ones((len(omega),len(tau),len(t)))     if n==1:         return np.cos(omega[:,np.newaxis,np.newaxis]                       *(t[np.newaxis,np.newaxis,:]-tau[np.newaxis,:,np.newaxis]))     if n==2:         return np.sin(omega[:,np.newaxis,np.newaxis]                       *(t[np.newaxis,np.newaxis,:]-tau[np.newaxis,:,np.newaxis]))      def make_phi(omega,tau,t):     phi0 = np.ones((len(omega),len(tau),len(t)))     phi1 = np.cos(omega[:,np.newaxis,np.newaxis]*(t[np.newaxis,np.newaxis,:]-tau[np.newaxis,:,np.newaxis]))     phi2 = np.sin(omega[:,np.newaxis,np.newaxis]*(t[np.newaxis,np.newaxis,:]-tau[np.newaxis,:,np.newaxis]))     return np.array([phi0,phi1,phi2])  # Eq. 4.2 def inner(f,g,omega,tau,t,weight,c=0.0125,weights=None):     tensor = ne.evaluate('weight*f*g')     return np.sum(tensor,axis=-1) / np.sum(weight,axis=-1)       # Eq. 4.3 def S_inv(omega,tau,t,weight,phi_array,c=0.0125,weights=None):     Sij = np.ones((len(omega),len(tau),3,3))     for i in range(3):         for j in range(3):             Sij[...,i,j] = inner(phi_array[i],phi_array[j],omega,tau,t,weight,c=c,weights=weights)     Sinv = np.linalg.inv(Sij)     return Sinv   def y(x,omega,tau,t,Sinv,weight,phi_array,c=0.0125,weights=None):     # returns matrix with shape (omega,tau,t)     yij = np.ones((len(omega),len(tau),3,3))     for i in range(3):         for j in range(3):             yij[...,i,j] = Sinv[...,i,j] * inner(phi_array[j],x,omega,tau,t,weight,c=c,weights=weights)     # Eq. 4.4     yi = np.sum(yij,axis=-1)     # Eq. 4.1     yt = np.zeros((len(omega),len(tau),len(t)))     for i in range(3):         yt += yi[...,i][...,np.newaxis] * phi_array[i]     return yt  # Eq. 5.9/10 def weighted_variation(x,omega,tau,t,weight,c=0.0125,weights=None):     # returns matrix with shape (omega, tau)     xx = inner(x,x,omega,tau,t,weight,c=c,weights=weights)     x1 = inner(1,x,omega,tau,t,weight,c=c,weights=weights)     #return xx-x1**2     return ne.evaluate('xx-x1**2')          # Eq. 5.12 def Z(x,omega,tau,t,Sinv,N,weight,phi_array,c=0.0125,weights=None):     Vx = weighted_variation(x,omega,tau,t,weight,c=c,weights=weights)     Vy = weighted_variation(y(x,omega,tau,t,Sinv,weight,phi_array,c=c,weights=weights),omega,tau,t,weight,c=c,weights=weights)     N[N&lt;3] = 3 #Stops WWZ from going negative for some sources     return ne.evaluate('(N-3) * Vy / (2 * (Vx-Vy))')  #Eq. 5.14 def A(x,omega,tau,t,c=0.0125,weights=None):     # returns matrix with shape (omega,tau)     yij = np.ones((len(omega),len(tau),3,3))     weight = wa(omega,tau,t,c=c,weights=weights)     Sinv = np.linalg.inv(S(omega,tau,t,c=c,weights=weights))     for i in range(3):         for j in range(3):             yij[...,i,j] = Sinv[...,i,j] * inner(phi(j,omega,tau,t),x,omega,tau,t,c=c,weights=weights)     # Eq. 4.4     yi = np.sum(yij,axis=-1)     y2 = yi[...,1][...]     y3 = yi[...,2][...]     return np.sqrt(np.square(y2)+np.square(y3)) <p>For this analysis I have been using a dataset of 1847 blazars observed by CHIME. I have the flux, error, and days saved in seperate files which I load in. I then use a sliding median filter to remove outliers and normalize the light curve such that it's mean is 0 and standard deviation is 1.</p> In\u00a0[104]: Copied! <pre>all_flux = np.loadtxt('all_flux.txt', delimiter=',') #The flux for each day for each source\nall_error = np.loadtxt('all_error.txt', delimiter=',') #The inverse-variance error for each day for each source\nall_days = np.loadtxt('all_days.txt', delimiter=',') #All CHIME days with data present\nsource_list = np.loadtxt('source_data/source_list',delimiter=',', dtype = str)#Gives me the names of the blazars\nall_dec = np.loadtxt('all_dec.txt')\n\n#Load a source, input is source_id, output is days, flux, and error\ndef load_source(source_id,normalized = True):\n    days = all_days\n    flux = all_flux[:,source_id]\n    error = all_error[:,source_id]\n    days = days\n    good_days = np.where(flux!=0)[0]\n    nan_days = np.where(np.isnan(flux[good_days])==True)[0]\n    good_days = np.delete(good_days, nan_days)\n    flux = flux[good_days]\n    error = error[good_days]\n    days = days[good_days]\n    if normalized:\n        flux = (flux - np.mean(flux)) / np.std(flux)\n    return days, flux, error\n\n#Uses Hampel filter to remove otliers\ndef remove_outliers(source_id):\n    days, flux, error = load_source(source_id,normalized = False)\n    ts = pd.Series(flux)\n    window=20\n    outlier_indices = hampel(ts, window_size=window, n=3) #Median test\n    filtered_days = np.delete(days,outlier_indices) \n    filtered_flux = np.delete(flux,outlier_indices)\n    filtered_error = np.delete(error,outlier_indices)\n    days_conversion = [np.any(all_days[i]==days[outlier_indices].astype(int)) for i in range(len(all_days))]\n    bad_days_indices = np.where(days_conversion)[0]\n    filtered_flux =( filtered_flux - np.mean(filtered_flux)) / np.std(filtered_flux) #normalize\n    return filtered_days, filtered_flux, filtered_error\n\n#Lomb-Scargle Periodogram for comparison with WWZ\ndef SNR_periodogram(days, flux,period_range=[4,200]):\n    y = flux\n    x = days\n    periods = np.linspace(period_range[0],period_range[1],period_range[1]-period_range[0]+1)\n    ls = np.zeros_like(periods)\n    for i, T in enumerate(periods):\n        f = 1/T\n        M = np.vstack((np.ones_like(x), np.sin(2*np.pi*f*x), np.cos(2*np.pi*f*x))).T\n        beta = np.linalg.lstsq(M, y, rcond=None)[0]\n        theta1, theta2 = beta[1], beta[2]\n        a = np.sqrt(theta1**2 + theta2**2)\n        sigma = np.std(np.dot(M,beta) - y)\n        SNR = a**2 / (2*sigma**2)\n        ls[i] = SNR\n    return periods, ls\n\n#If within the first/last 15 data points there are some seperated by a gap, I cut off the data before/after the gap\n#This is to fix an issue with the WWZ\ndef fix(days,flux,error,check=15):\n    if np.any(np.diff(days[:check]) &gt; 30):\n        cutoff = np.where(np.diff(days[:check])&gt;30)[0][0]+1\n        days = days[cutoff:]\n        flux = flux[cutoff:]\n        error = error[cutoff:]\n    last = len(days) - check\n    if np.any(np.diff(days[last:]) &gt; 30):\n        cutoff = len(days)-(check-np.where(np.diff(days[last:])&gt;30)[0][0])+1\n        days = days[:cutoff]\n        flux = flux[:cutoff]\n        error = error[:cutoff]\n    return days, flux, error\n</pre> all_flux = np.loadtxt('all_flux.txt', delimiter=',') #The flux for each day for each source all_error = np.loadtxt('all_error.txt', delimiter=',') #The inverse-variance error for each day for each source all_days = np.loadtxt('all_days.txt', delimiter=',') #All CHIME days with data present source_list = np.loadtxt('source_data/source_list',delimiter=',', dtype = str)#Gives me the names of the blazars all_dec = np.loadtxt('all_dec.txt')  #Load a source, input is source_id, output is days, flux, and error def load_source(source_id,normalized = True):     days = all_days     flux = all_flux[:,source_id]     error = all_error[:,source_id]     days = days     good_days = np.where(flux!=0)[0]     nan_days = np.where(np.isnan(flux[good_days])==True)[0]     good_days = np.delete(good_days, nan_days)     flux = flux[good_days]     error = error[good_days]     days = days[good_days]     if normalized:         flux = (flux - np.mean(flux)) / np.std(flux)     return days, flux, error  #Uses Hampel filter to remove otliers def remove_outliers(source_id):     days, flux, error = load_source(source_id,normalized = False)     ts = pd.Series(flux)     window=20     outlier_indices = hampel(ts, window_size=window, n=3) #Median test     filtered_days = np.delete(days,outlier_indices)      filtered_flux = np.delete(flux,outlier_indices)     filtered_error = np.delete(error,outlier_indices)     days_conversion = [np.any(all_days[i]==days[outlier_indices].astype(int)) for i in range(len(all_days))]     bad_days_indices = np.where(days_conversion)[0]     filtered_flux =( filtered_flux - np.mean(filtered_flux)) / np.std(filtered_flux) #normalize     return filtered_days, filtered_flux, filtered_error  #Lomb-Scargle Periodogram for comparison with WWZ def SNR_periodogram(days, flux,period_range=[4,200]):     y = flux     x = days     periods = np.linspace(period_range[0],period_range[1],period_range[1]-period_range[0]+1)     ls = np.zeros_like(periods)     for i, T in enumerate(periods):         f = 1/T         M = np.vstack((np.ones_like(x), np.sin(2*np.pi*f*x), np.cos(2*np.pi*f*x))).T         beta = np.linalg.lstsq(M, y, rcond=None)[0]         theta1, theta2 = beta[1], beta[2]         a = np.sqrt(theta1**2 + theta2**2)         sigma = np.std(np.dot(M,beta) - y)         SNR = a**2 / (2*sigma**2)         ls[i] = SNR     return periods, ls  #If within the first/last 15 data points there are some seperated by a gap, I cut off the data before/after the gap #This is to fix an issue with the WWZ def fix(days,flux,error,check=15):     if np.any(np.diff(days[:check]) &gt; 30):         cutoff = np.where(np.diff(days[:check])&gt;30)[0][0]+1         days = days[cutoff:]         flux = flux[cutoff:]         error = error[cutoff:]     last = len(days) - check     if np.any(np.diff(days[last:]) &gt; 30):         cutoff = len(days)-(check-np.where(np.diff(days[last:])&gt;30)[0][0])+1         days = days[:cutoff]         flux = flux[:cutoff]         error = error[:cutoff]     return days, flux, error In\u00a0[5]: Copied! <pre>#This function plots the WWZ of a specific source\ndef plot_wwz(days,flux,error=None,c=0.0125,tau_step=10,period_range=(4,200),title=\"WWZ\"):\n\n    \n    weights = 1/np.square(error) if np.all(error != None) else None\n    tau = days[::tau_step]\n    T = np.linspace(period_range[0],period_range[1],period_range[1]-period_range[0]+1)\n    omega = np.array([2*np.pi/T[ii] for ii in range(len(T))])\n\n    w = wa(omega,tau,days,c=c,weights=weights)\n    phi_array =  make_phi(omega,tau,days)\n    Sinv = S_inv(omega,tau,days,w,phi_array,c=c,weights=weights)\n    N = Neff(omega,tau,days,c=c,weights=weights)\n    wwz = Z(flux,omega,tau,days,Sinv,N,w,phi_array,c=c,weights=weights)\n\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2,wspace=0.0,hspace=0.0,width_ratios=[1, 3.6],height_ratios=[1, 3.6])\n    ((ax3,ax4),(ax1, ax2)) = gs.subplots(sharex=\"col\", sharey=False)\n    fig.delaxes(ax3)\n    fig.set_figheight(12)\n    fig.set_figwidth(14)\n    plt.set_cmap('cividis')\n    cm = ax2.pcolormesh(tau,2*np.pi/omega,wwz,shading='gouraud')\n    ax2.set_xlabel(\"Time (Days)\",fontsize=15)\n    ax2.tick_params(axis='x', labelsize=15)\n    ax2.tick_params(axis='y', labelsize=15)\n    plt.subplots_adjust(bottom=0.1, right=0.8, top=0.9)\n    cax = plt.axes([0.82, 0.15, 0.02, 0.5])\n    cbar = plt.colorbar(cm,cax=cax,shrink=0.5)\n    ax2.set_yticklabels([])\n    #cbar.set_label('Power', rotation=270,fontsize=15,labelpad=15)\n\n    periods, power = SNR_periodogram(days,flux,period_range=period_range)\n    ax1.plot(power,periods)\n    ax1.grid()\n    ax1.set_title(f\"L-S\",fontsize=18)\n    ax1.set_xlabel(\"Power\",fontsize=15)\n    ax1.set_ylabel(\"Period (Days)\",fontsize=15)\n    ax1.tick_params(axis='x', labelsize=15)\n    ax1.tick_params(axis='y', labelsize=15)\n    ax1.set_ylim(min(periods),max(periods))\n\n    ax4.set_title(title, fontsize=17,wrap=True)\n    ax4.errorbar(days,flux,yerr=error,fmt='.')\n    ax4.tick_params(axis='y', labelsize=15)\n    ax4.set_ylabel(\"Flux\",fontsize=17)\n    ax4.grid()\n    ax4.set_xlim(min(tau),max(tau))\n    plt.show()\n    return wwz\n</pre> #This function plots the WWZ of a specific source def plot_wwz(days,flux,error=None,c=0.0125,tau_step=10,period_range=(4,200),title=\"WWZ\"):           weights = 1/np.square(error) if np.all(error != None) else None     tau = days[::tau_step]     T = np.linspace(period_range[0],period_range[1],period_range[1]-period_range[0]+1)     omega = np.array([2*np.pi/T[ii] for ii in range(len(T))])      w = wa(omega,tau,days,c=c,weights=weights)     phi_array =  make_phi(omega,tau,days)     Sinv = S_inv(omega,tau,days,w,phi_array,c=c,weights=weights)     N = Neff(omega,tau,days,c=c,weights=weights)     wwz = Z(flux,omega,tau,days,Sinv,N,w,phi_array,c=c,weights=weights)      fig = plt.figure()     gs = fig.add_gridspec(2, 2,wspace=0.0,hspace=0.0,width_ratios=[1, 3.6],height_ratios=[1, 3.6])     ((ax3,ax4),(ax1, ax2)) = gs.subplots(sharex=\"col\", sharey=False)     fig.delaxes(ax3)     fig.set_figheight(12)     fig.set_figwidth(14)     plt.set_cmap('cividis')     cm = ax2.pcolormesh(tau,2*np.pi/omega,wwz,shading='gouraud')     ax2.set_xlabel(\"Time (Days)\",fontsize=15)     ax2.tick_params(axis='x', labelsize=15)     ax2.tick_params(axis='y', labelsize=15)     plt.subplots_adjust(bottom=0.1, right=0.8, top=0.9)     cax = plt.axes([0.82, 0.15, 0.02, 0.5])     cbar = plt.colorbar(cm,cax=cax,shrink=0.5)     ax2.set_yticklabels([])     #cbar.set_label('Power', rotation=270,fontsize=15,labelpad=15)      periods, power = SNR_periodogram(days,flux,period_range=period_range)     ax1.plot(power,periods)     ax1.grid()     ax1.set_title(f\"L-S\",fontsize=18)     ax1.set_xlabel(\"Power\",fontsize=15)     ax1.set_ylabel(\"Period (Days)\",fontsize=15)     ax1.tick_params(axis='x', labelsize=15)     ax1.tick_params(axis='y', labelsize=15)     ax1.set_ylim(min(periods),max(periods))      ax4.set_title(title, fontsize=17,wrap=True)     ax4.errorbar(days,flux,yerr=error,fmt='.')     ax4.tick_params(axis='y', labelsize=15)     ax4.set_ylabel(\"Flux\",fontsize=17)     ax4.grid()     ax4.set_xlim(min(tau),max(tau))     plt.show()     return wwz <p>The two plots below show some of the benefits of the WWZ over the Lomb-Scargle periodogram. It can detect signals with varying frequencies and also transient periodic signals.</p> In\u00a0[6]: Copied! <pre>t = np.linspace(0,1000,3000)\nx = np.sin(2*np.pi/(1+(t/2)**(4/7))*t)\nx += 0.1*np.random.randn(len(x))\nxerr = 0.1*np.ones_like(x)\nwwz = plot_wwz(t,x,xerr,period_range=(4,100),title=f\"WWZ of sine function with changing frequency\",c=1/(8*(1.27389**2)*np.pi**2))\n</pre> t = np.linspace(0,1000,3000) x = np.sin(2*np.pi/(1+(t/2)**(4/7))*t) x += 0.1*np.random.randn(len(x)) xerr = 0.1*np.ones_like(x) wwz = plot_wwz(t,x,xerr,period_range=(4,100),title=f\"WWZ of sine function with changing frequency\",c=1/(8*(1.27389**2)*np.pi**2)) In\u00a0[7]: Copied! <pre>t = np.linspace(0,1500,1500)\nx = np.exp(-t/300) * np.sin(2*np.pi/100*t)\nx += 0.1*np.random.randn(len(x))\nxerr = 0.1*np.ones_like(x)\nwwz = plot_wwz(t,x,xerr,period_range=(4,150),title=f\"WWZ of a decaying sine function\",c=1/(8*(1.27389**2)*np.pi**2))\n</pre> t = np.linspace(0,1500,1500) x = np.exp(-t/300) * np.sin(2*np.pi/100*t) x += 0.1*np.random.randn(len(x)) xerr = 0.1*np.ones_like(x) wwz = plot_wwz(t,x,xerr,period_range=(4,150),title=f\"WWZ of a decaying sine function\",c=1/(8*(1.27389**2)*np.pi**2)) In\u00a0[8]: Copied! <pre>def plot_wwz(days,flux,error=None,c=0.0125,tau_step=10,period_range=(4,200),title=\"WWZ\"):\n    weights = 1/np.square(error) if np.all(error != None) else None\n    tau = days[::tau_step]\n    T = np.linspace(period_range[0],period_range[1],period_range[1]-period_range[0]+1)\n    omega = np.array([2*np.pi/T[ii] for ii in range(len(T))])\n\n    w = wa(omega,tau,days,c=c,weights=weights)\n    phi_array =  make_phi(omega,tau,days)\n    Sinv = S_inv(omega,tau,days,w,phi_array,c=c,weights=weights)\n    N = Neff(omega,tau,days,c=c,weights=weights)\n    wwz = Z(flux,omega,tau,days,Sinv,N,w,phi_array,c=c,weights=weights)\n\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2,wspace=0.0,hspace=0.0,width_ratios=[1, 3.6],height_ratios=[1, 3.6])\n    ((ax3,ax4),(ax1, ax2)) = gs.subplots(sharex=\"col\", sharey=False)\n    fig.delaxes(ax3)\n    fig.set_figheight(12)\n    fig.set_figwidth(14)\n    plt.set_cmap('cividis')\n    cm = ax2.pcolormesh(tau,2*np.pi/omega,wwz,shading='gouraud')\n    ax2.set_xlabel(\"Time (Days)\",fontsize=15)\n    ax2.tick_params(axis='x', labelsize=15)\n    ax2.tick_params(axis='y', labelsize=15)\n    plt.subplots_adjust(bottom=0.1, right=0.8, top=0.9)\n    cax = plt.axes([0.82, 0.15, 0.02, 0.5])\n    cbar = plt.colorbar(cm,cax=cax,shrink=0.5)\n    ax2.set_yticklabels([])\n    #cbar.set_label('Power', rotation=270,fontsize=15,labelpad=15)\n\n    periods, power = SNR_periodogram(days,flux,period_range=period_range)\n    ax1.plot(power,periods)\n    \n    \n    ax1_2=ax1.twiny()\n    ax1_2.plot([max(wwz[i,:]) for i in range(len(omega))],periods,color='tab:orange')\n    ax1_2.tick_params(axis='x', labelsize=15)\n    ax1.grid()\n    ax1.set_title(f\"L-S\",fontsize=18)\n    ax1.set_xlabel(\"Power\",fontsize=15)\n    ax1.set_ylabel(\"Period (Days)\",fontsize=15)\n    ax1.tick_params(axis='x', labelsize=15)\n    ax1.tick_params(axis='y', labelsize=15)\n    ax1.set_ylim(min(periods),max(periods))\n\n    ax4.set_title(title, fontsize=17,wrap=True)\n    ax4.errorbar(days,flux,yerr=error,fmt='.')\n    ax4.tick_params(axis='y', labelsize=15)\n    ax4.set_ylabel(\"Flux\",fontsize=17)\n    ax4.grid()\n    ax4.set_xlim(min(tau),max(tau))\n    plt.show()\n    return wwz\n#TEMP CELL\n\ndef window(t,c,omega,tau):\n    np.exp(-c*omega**2*(t-tau)**2)\n</pre> def plot_wwz(days,flux,error=None,c=0.0125,tau_step=10,period_range=(4,200),title=\"WWZ\"):     weights = 1/np.square(error) if np.all(error != None) else None     tau = days[::tau_step]     T = np.linspace(period_range[0],period_range[1],period_range[1]-period_range[0]+1)     omega = np.array([2*np.pi/T[ii] for ii in range(len(T))])      w = wa(omega,tau,days,c=c,weights=weights)     phi_array =  make_phi(omega,tau,days)     Sinv = S_inv(omega,tau,days,w,phi_array,c=c,weights=weights)     N = Neff(omega,tau,days,c=c,weights=weights)     wwz = Z(flux,omega,tau,days,Sinv,N,w,phi_array,c=c,weights=weights)      fig = plt.figure()     gs = fig.add_gridspec(2, 2,wspace=0.0,hspace=0.0,width_ratios=[1, 3.6],height_ratios=[1, 3.6])     ((ax3,ax4),(ax1, ax2)) = gs.subplots(sharex=\"col\", sharey=False)     fig.delaxes(ax3)     fig.set_figheight(12)     fig.set_figwidth(14)     plt.set_cmap('cividis')     cm = ax2.pcolormesh(tau,2*np.pi/omega,wwz,shading='gouraud')     ax2.set_xlabel(\"Time (Days)\",fontsize=15)     ax2.tick_params(axis='x', labelsize=15)     ax2.tick_params(axis='y', labelsize=15)     plt.subplots_adjust(bottom=0.1, right=0.8, top=0.9)     cax = plt.axes([0.82, 0.15, 0.02, 0.5])     cbar = plt.colorbar(cm,cax=cax,shrink=0.5)     ax2.set_yticklabels([])     #cbar.set_label('Power', rotation=270,fontsize=15,labelpad=15)      periods, power = SNR_periodogram(days,flux,period_range=period_range)     ax1.plot(power,periods)               ax1_2=ax1.twiny()     ax1_2.plot([max(wwz[i,:]) for i in range(len(omega))],periods,color='tab:orange')     ax1_2.tick_params(axis='x', labelsize=15)     ax1.grid()     ax1.set_title(f\"L-S\",fontsize=18)     ax1.set_xlabel(\"Power\",fontsize=15)     ax1.set_ylabel(\"Period (Days)\",fontsize=15)     ax1.tick_params(axis='x', labelsize=15)     ax1.tick_params(axis='y', labelsize=15)     ax1.set_ylim(min(periods),max(periods))      ax4.set_title(title, fontsize=17,wrap=True)     ax4.errorbar(days,flux,yerr=error,fmt='.')     ax4.tick_params(axis='y', labelsize=15)     ax4.set_ylabel(\"Flux\",fontsize=17)     ax4.grid()     ax4.set_xlim(min(tau),max(tau))     plt.show()     return wwz #TEMP CELL  def window(t,c,omega,tau):     np.exp(-c*omega**2*(t-tau)**2) <p>Now, let us plot the WWZ for a real source observed by CHIME. Notice the increase in power towards the second half of the lightcurve, where the periodic signal appears stronger. It is not a coincidence that the peak in power is right at a period of 365 days. This occurs for many sources, due to some seasonal effects on CHIME. Due to this, for the actual analysis we focus on periods shorter than 100 days. Because CHIME observes all 1847 blazars every day, shorter time-scale variation is where CHIME is strong anyway.</p> In\u00a0[9]: Copied! <pre>source_id = 625\nsource_name = source_list[source_id]\ndays, flux, error = remove_outliers(source_id)\ndays, flux, error = fix(days,flux,error)\nwwz = plot_wwz(days,flux,error,period_range=(4,700),title=f\"{source_name} WWZ\",c=1/(8*(1.27389**2)*np.pi**2))\n</pre> source_id = 625 source_name = source_list[source_id] days, flux, error = remove_outliers(source_id) days, flux, error = fix(days,flux,error) wwz = plot_wwz(days,flux,error,period_range=(4,700),title=f\"{source_name} WWZ\",c=1/(8*(1.27389**2)*np.pi**2)) <p>Blazar variability, in general, exhibits red-noise-like behavior. This stochaistic variation can sometimes be misidentified as quasi-periodic oscillations (QPOs). Therefore, for a rigorous estimation of the QPO features, we must compare the WWZ of our blazar lightcurves with purely stochastic simulated lightcurves. For this project, blazar light curves are modelled as a continuous time first-order autoregressive process (CAR(1)), which are commonly used to model AGN stochastic variability. A CAR(1) process exhibits the Markov property, which implies that the next data point in the stochastic light curve depends solely on the present data point and not on any preceding values. In other words, the modelling of the stochastic light curve is based on a Markovian assumption, whereby each data point is conditionally independent of all prior data points given the present data point. We can therefore express the conditional expected value of $Y(t)$ given $Y(s)$ for $s &lt; t$ as</p> <p>\\begin{equation} E(Y(t)|Y(s)) = e^{-\\Delta t/\\tau}Y(s)+q(1-e^{-\\Delta t/\\tau}), \\end{equation} and the conditional variance in $Y(t)$ given $Y(s)$ is given by \\begin{equation} \\text{Var}(Y(t)|Y(s)) = \\frac{\\tau\\sigma^2}{2}(1-e^{-2\\Delta t/\\tau}), \\end{equation} where $\\Delta t = t-s$, $\\sigma$ is the variation amplitude at long timescale, and $\\tau$ is the characteristic damping timescale, and $q$ is the mean value of the light-curve.</p> <p>By using this model with values of $\\sigma$ and $\\tau$ that are consistent with our original data, we can generate full light curves with the same variation amplitude and timescale as the blazar we are observing. In the next section, we will estimate these parameters from real light curves collected by CHIME.</p> <p>We use the following likelihood function to model a CAR(1) process, \\begin{equation} p(y_1,...,y_n |\\sigma,\\tau,q) = \\prod_{i=1}^{n}[2\\pi(\\Omega_i+\\sigma^2)]^{-\\frac{1}{2}} \\times \\exp{\\left[-\\frac{1}{2}\\frac{(\\hat{y_i}-y^*_i)^2}{\\Omega_i+\\sigma^2_i}\\right]}, \\label{eqn:likelihood} \\end{equation} where $$y^*_i = y_i - q,$$ $$\\hat{y_i} = e^{-(t_i -t_{i-1})/\\tau}\\hat{y}_{i-1} + \\frac{e^{-(t_i -t_{i-1})/\\tau}\\Omega_{i-1}}{\\Omega_{i-1}+\\sigma^2_{i-1}}(y^*_{i-1}-\\hat{y}_{i-1}),$$ and $$\\Omega_i = \\Omega_1(1-e^{-2(t_i -t_{i-1})/\\tau})+e^{-2(t_i -t_{i-1})/\\tau}\\Omega_{i-1}\\left(1-\\frac{\\Omega_{i-1}}{\\Omega_{i-1}+\\sigma^2_{i-1}}\\right).$$ The initial values are $\\Omega_1 = \\frac{\\tau \\sigma^2}{2}$ and $\\hat{x}_1 = 0.$</p> <p>We set a uniform prior for the logarithm of the parameters $\\sigma$ and $\\tau$ and a uniform prior for q and normalize our light curve by subtracting the mean and dividing by the standard deviation. We then apply the Markov Chain Monte Carlo (MCMC) sampler $\\texttt{emcee}$ with 36 walkers and 5000 iterations to create histograms for each of the three parameters. In order to simulate a light curve with the same characteristics as the observed data from CHIME, we simply have to draw from the sample distributions of each parameter and construct the light curve using the process described in the previous cell.</p> In\u00a0[10]: Copied! <pre>def simulate(theta, days, error):\n    # We simulate the lightcurve as uniformly sampled, then mask it the same as our data\n    time = np.linspace(days[0], days[-1], 1 + int(days[-1] - days[0]))\n    day_indices = np.where(np.isin(time, days))\n    delta_t = 1\n    sigma, tau, q = theta\n    y = np.empty(len(time))\n    y[0] = q  # initial value is mean\n    # using equations from Brockwell &amp; Davis\n    for i in range(len(time)-1):\n        mean = np.exp(-delta_t / tau) * y[i] + q * (1 - np.exp(-delta_t / tau))\n        sd = np.sqrt((tau * sigma ** 2) / 2 * (1 - np.exp(-2 * delta_t / tau)))\n        y[i + 1] = np.random.normal(loc=mean, scale=sd)\n    y = y[day_indices]\n    time = time[day_indices]\n    return time, y, error\n\n#Defining probability distributions for MCMC\ndef log_prior(theta):\n    log_sigma, log_tau, q = theta\n    if -10 &lt; log_sigma &lt; 10 and 0 &lt; log_tau &lt; 10 and -100 &lt; q &lt; 100:\n        return 0.0\n    return -np.inf\n\ndef log_likelihood(theta,time,y,yerr):\n    log_sigma, log_tau, q = theta\n    sigma = np.exp(log_sigma)\n    tau = np.exp(log_tau)\n    y_star = y - q\n    y_hat = np.zeros_like(y)\n    Omega = np.zeros_like(y)\n    a = np.exp(-np.diff(time)/tau)\n    a = np.insert(a, 0, 0)  # Add a zero at the end to match the size of y_hat and Omega\n    Omega[0] = (tau * sigma**2)/2\n    Omega[1:] = Omega[0]*(1-a[1:]**2) + a[1:]**2 * Omega[1:] * (1 - Omega[1:]/(Omega[1:]+yerr[1:]**2))\n    y_hat[1:] = a[1:] * y_hat[:-1] + a[1:] * Omega[:-1]/(Omega[:-1]+yerr[:-1]**2) * (y_star[:-1]-y_hat[:-1])\n    return -0.5 * np.sum(np.log(2 * np.pi * (Omega+yerr**2))) - 0.5 * np.sum( (y_hat - y_star)**2 / (Omega + yerr**2) )\n\ndef log_posterior(theta,time,y,yerr):\n    return log_prior(theta) + log_likelihood(theta,time,y,yerr)\n</pre> def simulate(theta, days, error):     # We simulate the lightcurve as uniformly sampled, then mask it the same as our data     time = np.linspace(days[0], days[-1], 1 + int(days[-1] - days[0]))     day_indices = np.where(np.isin(time, days))     delta_t = 1     sigma, tau, q = theta     y = np.empty(len(time))     y[0] = q  # initial value is mean     # using equations from Brockwell &amp; Davis     for i in range(len(time)-1):         mean = np.exp(-delta_t / tau) * y[i] + q * (1 - np.exp(-delta_t / tau))         sd = np.sqrt((tau * sigma ** 2) / 2 * (1 - np.exp(-2 * delta_t / tau)))         y[i + 1] = np.random.normal(loc=mean, scale=sd)     y = y[day_indices]     time = time[day_indices]     return time, y, error  #Defining probability distributions for MCMC def log_prior(theta):     log_sigma, log_tau, q = theta     if -10 &lt; log_sigma &lt; 10 and 0 &lt; log_tau &lt; 10 and -100 &lt; q &lt; 100:         return 0.0     return -np.inf  def log_likelihood(theta,time,y,yerr):     log_sigma, log_tau, q = theta     sigma = np.exp(log_sigma)     tau = np.exp(log_tau)     y_star = y - q     y_hat = np.zeros_like(y)     Omega = np.zeros_like(y)     a = np.exp(-np.diff(time)/tau)     a = np.insert(a, 0, 0)  # Add a zero at the end to match the size of y_hat and Omega     Omega[0] = (tau * sigma**2)/2     Omega[1:] = Omega[0]*(1-a[1:]**2) + a[1:]**2 * Omega[1:] * (1 - Omega[1:]/(Omega[1:]+yerr[1:]**2))     y_hat[1:] = a[1:] * y_hat[:-1] + a[1:] * Omega[:-1]/(Omega[:-1]+yerr[:-1]**2) * (y_star[:-1]-y_hat[:-1])     return -0.5 * np.sum(np.log(2 * np.pi * (Omega+yerr**2))) - 0.5 * np.sum( (y_hat - y_star)**2 / (Omega + yerr**2) )  def log_posterior(theta,time,y,yerr):     return log_prior(theta) + log_likelihood(theta,time,y,yerr)  <p>For each source we simulate 10000 lightcurves, and apply the same sampling as the original lightcurve to them. Then for each period on the WWZ, we compare the power reached by the original WWZ at any time, to the maximum power reached by the simulated lightcurve's WWZ at any time.</p> <p>In order to find significant periodic candidates we must simulate ~100000 lightcurves for each source. For we source we are testing 100 different periods $$1847\\cdot100 = 184700$$ $$184700\\cdot\\frac{1}{100000} \\approx 1.85$$ Meaning we should expect around 1 or 2 false alarms. If more than 2 individual periods are identified, then they are significant at face value using Gaussian statistics and should therefore be studied further. Below is the test I run every source through.</p> In\u00a0[11]: Copied! <pre>def significance_test(source_id, num_sim=100000, c=1/(8*(1.27389**2)*np.pi**2)):\n    days, flux, error = remove_outliers(source_id)\n    days, flux, error = fix(days, flux, error, check=15)\n\n    weights = 1/np.square(error)\n    tau = days[::10]\n    T = np.linspace(4, 100, 97)\n    omega = np.array([2*np.pi/T[ii] for ii in range(len(T))])\n    w = wa(omega, tau, days, c=c, weights=weights)\n    phi_array = make_phi(omega, tau, days)\n    Sinv = S_inv(omega, tau, days, w, phi_array, c=c, weights=weights)\n    N = Neff(omega, tau, days, c=c, weights=weights)\n    wwz = Z(flux, omega, tau, days, Sinv, N,\n            w, phi_array, c=c, weights=weights)\n\n    ############################ MCMC ############################\n    nwalkers = 36\n    ndim = 3\n    steps = 5000\n\n    # Creates a grid with different starting values of sigma and tau that span across parameter space.\n    lnt = np.linspace(0, 5, 6)\n    lns = np.linspace(-5, 5, 6)\n    xv, yv = np.meshgrid(lns, lnt)\n    pos = np.zeros((36, 3))\n    pos[:, 0] = xv.reshape(-1)\n    pos[:, 1] = yv.reshape(-1)\n    # Did this so that all of the q walkers dont start at 0 exactly.\n    pos = pos + 1e-4 * np.random.randn(nwalkers, ndim)\n\n    sampler = emcee.EnsembleSampler(\n        nwalkers, ndim, log_posterior, args=(days, flux, error))\n    sampler.run_mcmc(pos, steps, progress=False)\n\n    flat_samples = sampler.get_chain(discard=100, thin=15, flat=True)\n\n    ############################ Simulation ############################\n    all_simulations = list()\n    for i in range(num_sim):\n        sigma_rand, tau_rand, q_rand = random_param(flat_samples)\n        theta = [sigma_rand, tau_rand, q_rand]\n        time, x, error = simulate(theta, days, error)\n        single_sim = Z(x, omega, tau, time, Sinv, N, w,\n                       phi_array, c=c, weights=weights)\n        all_simulations.append(single_sim)\n    all_simulations = np.array(all_simulations)\n\n    sig_periods = list()\n    for i in range(len(omega)):\n        if np.any(wwz[i, :] &gt; max(all_simulations[:, i, :].flatten())):\n            sig_periods.append(T[i])\n    return sorted(sig_periods)\n</pre> def significance_test(source_id, num_sim=100000, c=1/(8*(1.27389**2)*np.pi**2)):     days, flux, error = remove_outliers(source_id)     days, flux, error = fix(days, flux, error, check=15)      weights = 1/np.square(error)     tau = days[::10]     T = np.linspace(4, 100, 97)     omega = np.array([2*np.pi/T[ii] for ii in range(len(T))])     w = wa(omega, tau, days, c=c, weights=weights)     phi_array = make_phi(omega, tau, days)     Sinv = S_inv(omega, tau, days, w, phi_array, c=c, weights=weights)     N = Neff(omega, tau, days, c=c, weights=weights)     wwz = Z(flux, omega, tau, days, Sinv, N,             w, phi_array, c=c, weights=weights)      ############################ MCMC ############################     nwalkers = 36     ndim = 3     steps = 5000      # Creates a grid with different starting values of sigma and tau that span across parameter space.     lnt = np.linspace(0, 5, 6)     lns = np.linspace(-5, 5, 6)     xv, yv = np.meshgrid(lns, lnt)     pos = np.zeros((36, 3))     pos[:, 0] = xv.reshape(-1)     pos[:, 1] = yv.reshape(-1)     # Did this so that all of the q walkers dont start at 0 exactly.     pos = pos + 1e-4 * np.random.randn(nwalkers, ndim)      sampler = emcee.EnsembleSampler(         nwalkers, ndim, log_posterior, args=(days, flux, error))     sampler.run_mcmc(pos, steps, progress=False)      flat_samples = sampler.get_chain(discard=100, thin=15, flat=True)      ############################ Simulation ############################     all_simulations = list()     for i in range(num_sim):         sigma_rand, tau_rand, q_rand = random_param(flat_samples)         theta = [sigma_rand, tau_rand, q_rand]         time, x, error = simulate(theta, days, error)         single_sim = Z(x, omega, tau, time, Sinv, N, w,                        phi_array, c=c, weights=weights)         all_simulations.append(single_sim)     all_simulations = np.array(all_simulations)      sig_periods = list()     for i in range(len(omega)):         if np.any(wwz[i, :] &gt; max(all_simulations[:, i, :].flatten())):             sig_periods.append(T[i])     return sorted(sig_periods) In\u00a0[12]: Copied! <pre>sig_periods = np.load('sig_test_max_periods_100000.npy',allow_pickle=True)\n</pre> sig_periods = np.load('sig_test_max_periods_100000.npy',allow_pickle=True) In\u00a0[13]: Copied! <pre>sig_sources = list()\nfor i in range(len(sig_periods)):\n    if sig_periods[i]:\n        sig_sources.append(i)\nprint(sig_sources)\n</pre> sig_sources = list() for i in range(len(sig_periods)):     if sig_periods[i]:         sig_sources.append(i) print(sig_sources)  <pre>[518, 660, 864, 878, 1268, 1762]\n</pre> <p>The above test identifies 6 sources as significant, with the requirement that two adjacent periods must be significant in order for a source to be flagged.</p> <p>In order to run this test, with this many simulations, I made use of the Message Passing Interface (MPI) on Cedar. Specifically, I used the mpi4py.futures package. This package provides a high-level interface for asynchronously executing callables on a pool of worker processes using MPI for inter-process communication. With this Jupyter Notebook I will attach the python and bash scripts I used to execute the tests.</p> In\u00a0[14]: Copied! <pre>source_id = 518\nsource_name = source_list[source_id]\ndays, flux, error = remove_outliers(source_id)\ndays, flux, error = fix(days,flux,error)\nwwz = plot_wwz(days,flux,error,period_range=(4,100),title=f\"{source_name} WWZ\",c=1/(8*(1.27389**2)*np.pi**2))\n</pre> source_id = 518 source_name = source_list[source_id] days, flux, error = remove_outliers(source_id) days, flux, error = fix(days,flux,error) wwz = plot_wwz(days,flux,error,period_range=(4,100),title=f\"{source_name} WWZ\",c=1/(8*(1.27389**2)*np.pi**2)) <p>In order to determine whether or not the periodic signal detected is characteristic of the source itself or if it is coming from something in the background, we can look at the ringmap. I take the average of a circle of points in the sky and compute the WWZ for it to check if the signal detected previously is present.</p> In\u00a0[15]: Copied! <pre>all_flux_ringmap = np.load('ringmap_flux_518.npy',allow_pickle=True)\nall_error_ringmap = np.load('ringmap_error_518.npy',allow_pickle=True)\nra = np.load('ringmap_ra_518.npy',allow_pickle=True)\ndec = np.load('ringmap_dec_518.npy',allow_pickle=True)\n\ndef load_ringmap(ra_idx,dec_idx,normalized = True):\n    days = all_days\n    flux = all_flux_ringmap[:,ra_idx,dec_idx]\n    error = all_error_ringmap[:,ra_idx]\n    good_days = np.where(flux!=0)[0]\n    nan_days = np.where(np.isnan(flux[good_days])==True)[0]\n    good_days = np.delete(good_days, nan_days)\n    flux = flux[good_days]\n    error = error[good_days]\n    days = days[good_days]\n    if normalized:\n        flux = (flux - np.mean(flux)) / np.std(flux)\n    return days, flux, error\n\ndef remove_outliers_ringmap(ra_idx,dec_idx):\n    days, flux, error = load_ringmap(ra_idx,dec_idx,normalized = False)\n    ts = pd.Series(flux)\n    window=20\n    outlier_indices = hampel(ts, window_size=window, n=3) #Median test\n    filtered_days = np.delete(days,outlier_indices) \n    filtered_flux = np.delete(flux,outlier_indices)\n    filtered_error = np.delete(error,outlier_indices)\n    days_conversion = [np.any(all_days[i]==days[outlier_indices].astype(int)) for i in range(len(all_days))]\n    bad_days_indices = np.where(days_conversion)[0]\n    filtered_flux =( filtered_flux - np.mean(filtered_flux)) / np.std(filtered_flux) #normalize\n    return filtered_days, filtered_flux, filtered_error\n\ndef flatten(matrix):\n    flat_list = []\n    for row in matrix:\n        flat_list += row\n    return flat_list\n</pre> all_flux_ringmap = np.load('ringmap_flux_518.npy',allow_pickle=True) all_error_ringmap = np.load('ringmap_error_518.npy',allow_pickle=True) ra = np.load('ringmap_ra_518.npy',allow_pickle=True) dec = np.load('ringmap_dec_518.npy',allow_pickle=True)  def load_ringmap(ra_idx,dec_idx,normalized = True):     days = all_days     flux = all_flux_ringmap[:,ra_idx,dec_idx]     error = all_error_ringmap[:,ra_idx]     good_days = np.where(flux!=0)[0]     nan_days = np.where(np.isnan(flux[good_days])==True)[0]     good_days = np.delete(good_days, nan_days)     flux = flux[good_days]     error = error[good_days]     days = days[good_days]     if normalized:         flux = (flux - np.mean(flux)) / np.std(flux)     return days, flux, error  def remove_outliers_ringmap(ra_idx,dec_idx):     days, flux, error = load_ringmap(ra_idx,dec_idx,normalized = False)     ts = pd.Series(flux)     window=20     outlier_indices = hampel(ts, window_size=window, n=3) #Median test     filtered_days = np.delete(days,outlier_indices)      filtered_flux = np.delete(flux,outlier_indices)     filtered_error = np.delete(error,outlier_indices)     days_conversion = [np.any(all_days[i]==days[outlier_indices].astype(int)) for i in range(len(all_days))]     bad_days_indices = np.where(days_conversion)[0]     filtered_flux =( filtered_flux - np.mean(filtered_flux)) / np.std(filtered_flux) #normalize     return filtered_days, filtered_flux, filtered_error  def flatten(matrix):     flat_list = []     for row in matrix:         flat_list += row     return flat_list In\u00a0[16]: Copied! <pre>fig = plt.figure(figsize=(11, 7))\nplt.title(f\"Averaged Points\", fontsize=20)\ncm = plt.pcolormesh(ra, dec, all_flux_ringmap[1].T, shading='gouraud')\ncbar = plt.colorbar(cm)\ncbar.set_label('Flux (Jy)', rotation=270, fontsize=15, labelpad=15)\nplt.xlabel(r'Right Ascension (deg)', fontsize=15)\nplt.ylabel('Declination (deg)', fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.plot(ra[20], dec[10], marker='*', markersize=15, color='yellow')\n\na, b = 20, 10  # Centre pixel\nr = 5  # Radius of circle\nEPSILON = 2\nindexes = list()\nfor yi in range(len(dec)):\n    for xi in range(len(ra)):\n        # see if we're close to (x-a)**2 + (y-b)**2 == r**2\n        if abs((xi-a)**2 + (yi-b)**2 - r**2) &lt; EPSILON**2:\n            plt.plot(ra[xi], dec[yi], marker='*',\n                     markersize=15, color='orange')\n            indexes.append((xi, yi))\nplt.show()\n</pre> fig = plt.figure(figsize=(11, 7)) plt.title(f\"Averaged Points\", fontsize=20) cm = plt.pcolormesh(ra, dec, all_flux_ringmap[1].T, shading='gouraud') cbar = plt.colorbar(cm) cbar.set_label('Flux (Jy)', rotation=270, fontsize=15, labelpad=15) plt.xlabel(r'Right Ascension (deg)', fontsize=15) plt.ylabel('Declination (deg)', fontsize=15) plt.xticks(fontsize=15) plt.yticks(fontsize=15) plt.plot(ra[20], dec[10], marker='*', markersize=15, color='yellow')  a, b = 20, 10  # Centre pixel r = 5  # Radius of circle EPSILON = 2 indexes = list() for yi in range(len(dec)):     for xi in range(len(ra)):         # see if we're close to (x-a)**2 + (y-b)**2 == r**2         if abs((xi-a)**2 + (yi-b)**2 - r**2) &lt; EPSILON**2:             plt.plot(ra[xi], dec[yi], marker='*',                      markersize=15, color='orange')             indexes.append((xi, yi)) plt.show() In\u00a0[17]: Copied! <pre>flux_dicts = list()\nerror_dicts = list()\n\nmax_days = list()\nfor pair in indexes:\n    np.zeros(len(days))\n    ra_idx, dec_idx = pair\n    days, flux, error = remove_outliers_ringmap(ra_idx, dec_idx)\n    days, flux, error = fix(days, flux, error)\n    flux_dicts.append(dict(zip(days, flux)))\n    error_dicts.append(dict(zip(days, error)))\n    max_days.append(list(days))\n\nmax_days = sorted(set(flatten(max_days)))\n\n\ndflux = defaultdict(list)\nfor d in flux_dicts:\n    for key, value in d.items():\n        dflux[key].append(value)\nfor key in dflux:\n    dflux[key] = np.mean(dflux[key])\n\nderror = defaultdict(list)\nfor d in error_dicts:\n    for key, value in d.items():\n        derror[key].append(value)\nfor key in derror:\n    derror[key] = np.mean(derror[key])\n\n\navg_flux = list(dflux.values())\navg_error = list(derror.values())\nwwz = plot_wwz(np.array(max_days), np.array(avg_flux), np.array(avg_error), period_range=(\n    4, 100), title=f\"Average of Circle WWZ\", c=1/(8*(1.27389**2)*np.pi**2))\n</pre> flux_dicts = list() error_dicts = list()  max_days = list() for pair in indexes:     np.zeros(len(days))     ra_idx, dec_idx = pair     days, flux, error = remove_outliers_ringmap(ra_idx, dec_idx)     days, flux, error = fix(days, flux, error)     flux_dicts.append(dict(zip(days, flux)))     error_dicts.append(dict(zip(days, error)))     max_days.append(list(days))  max_days = sorted(set(flatten(max_days)))   dflux = defaultdict(list) for d in flux_dicts:     for key, value in d.items():         dflux[key].append(value) for key in dflux:     dflux[key] = np.mean(dflux[key])  derror = defaultdict(list) for d in error_dicts:     for key, value in d.items():         derror[key].append(value) for key in derror:     derror[key] = np.mean(derror[key])   avg_flux = list(dflux.values()) avg_error = list(derror.values()) wwz = plot_wwz(np.array(max_days), np.array(avg_flux), np.array(avg_error), period_range=(     4, 100), title=f\"Average of Circle WWZ\", c=1/(8*(1.27389**2)*np.pi**2))"},{"location":"projects/python_project/WWZ/#detectings-qpos-using-weighted-wavelet-z-transform-wwz","title":"Detectings QPOs Using Weighted Wavelet Z-Transform (WWZ)\u00b6","text":"<p>In this notebook we search for quasi-periodic oscillations (QPOs) in blazar light curves using the method of weighted wavelet Z-transform. The WWZ is a time-frequency analysis method, exploring both the frequency domain and the time domain. This has the advantage of allowing us to detect short-lived periodicity that may not be present for the entire duration of observation.</p> <p>The WWZ is defined as the following, $$Z = \\frac{ (N_{\\text{eff}}-3)V_y }{ 2(V_x-V_y) }$$</p> <p>Each term, and how this formula came about is explained best in Foster(1996) but a quick overview is shown below.</p> <p>Link to Foster(1996) : https://articles.adsabs.harvard.edu//full/1996AJ....112.1709F/0001709.000.html</p>"},{"location":"projects/python_project/WWZ/#ringmap-analysis","title":"Ringmap Analysis\u00b6","text":""},{"location":"publications/","title":"Publications","text":""},{"location":"publications/#2024","title":"2024","text":"<ul> <li>Blazar Variability Analysis (In Progress), Davies-Velie, E &amp; Wulf, D 2024</li> <li>Fake Paper #1 Davies-Velie et al. 2024</li> </ul>"},{"location":"publications/#2022","title":"2022","text":"<ul> <li>Fake Paper #2 Davies-Velie et al. 2022</li> </ul>"},{"location":"publications/#2002","title":"2002","text":"<ul> <li>\"faslkfjsalkfjew Davies-Velie et al. 2002</li> </ul>"},{"location":"reasearch/","title":"Research Interests","text":"<p>My research interests are in the realm of radio astronomy. I am interested in the intersection of data analysis and instrumentation. Currently I am focussed on using FRBs as cosmological probes as well as comissioning the upcoming CHORD radio telescope. More details are provided below. </p>"},{"location":"reasearch/#projects","title":"Projects","text":""},{"location":"reasearch/#canadian-hydrogen-intensity-mapping-experiment-chime","title":"Canadian Hydrogen Intensity Mapping Experiment (CHIME)","text":"<p>Blazar Analysis, 2023\u2013Present Supervisors: Matt Dobbs and Dallas Wulf</p> <p>My work with CHIME initially centered on point source analysis of blazars \u2014 highly energetic Active Galactic Nuclei (AGN) that often display significant variability, including quasi-periodic oscillations (QPOs). I led the development of an automated pipeline to detect variability signatures in a large catalog of blazars, leveraging CHIME\u2019s broad sky coverage to provide insights into AGN dynamics and emission mechanisms. In this project, I implemented the Lomb-Scargle Periodogram and Weighted Wavelet Z-Transform from scratch, demonstrating proficiency in Python programming and signal analysis. Additionally, I employed Monte Carlo methods to quantify the significance of dominant periods, modeling blazar light curves as autoregressive processes and refining my parallelization skills to handle the dataset efficiently on compute clusters. Currently, I am co-authoring a paper on blazar variability analysis with Dallas Wulf. For details on this project you can check out the presentation I gave to the CHIME collaboration here</p> <p>FRB Analysis 2024-Present Supervisor: Matt Dobbs</p> <p>Fast Radio Bursts (FRBs) are a promising new tool for probing the ionization history of the universe, including the reionization of helium. Because FRBs are highly dispersed as they travel through the ionized intergalactic medium (IGM), their dispersion measure (DM) \u2014 the integrated column density of free electrons along the line of sight \u2014 serves as a probe of the ionization state of the IGM at different redshifts.</p> <p>The reionization history of the universe involves two main phases:</p> <ul> <li>Hydrogen Reionization: Occurred around \\(z \\approx 6\\) when ionizing photons from early stars and galaxies fully ionized hydrogen.</li> <li>Helium Reionization: This second stage occurred later, at around \\(z \\approx 3\\), when high-energy radiation from quasars reionized singly ionized helium (He II), converting it into fully ionized helium (He III).</li> </ul> <p>This process significantly influenced the thermal and ionization state of the IGM, especially around \\(z \\approx 3\\), leaving an imprint that can be detected by examining FRB dispersion measures at these redshifts.</p> <p>The observed dispersion measure, \\(\\mathrm{DM}_{\\mathrm{IGM}}\\), for an extragalactic FRB can be written as the integral of the electron density along the line of sight:</p> \\[ \\mathrm{DM}_{\\mathrm{IGM}} = \\int_0^z \\frac{n_e(z')}{1+z'} \\, d\\ell. \\] <p>In a flat \\(\\Lambda\\)CDM cosmology, the relation between proper length element \\(d\\ell\\) and redshift \\(z\\) is given by:</p> \\[ d\\ell = \\frac{1}{1+z}\\frac{c}{H_0} \\frac{dz}{\\sqrt{\\Omega_m(1+z)^3 + \\Omega_\\Lambda}}, \\] <p>where \\(c\\) is the speed of light, \\(H_0\\) is the Hubble constant, \\(\\Omega_m\\) is the present-day matter density, and \\(\\Omega_\\Lambda = 1 - \\Omega_m\\) is the vacuum energy density. The electron density can be expressed as:</p> \\[ n_e(z) = \\frac{\\rho_{c,0} \\, \\Omega_b \\, f_{\\mathrm{IGM}} \\, X_e(z)}{m_p}(1+z)^3, \\] <p>where \\(\\rho_{c,0} = \\frac{3 H_0^2}{8 \\pi G}\\) is the critical density of the Universe, \\(m_p\\) is the proton mass, \\(\\Omega_b\\) is the baryon density parameter, and \\(f_{\\mathrm{IGM}} \\simeq 0.83\\) is the mass fraction of baryons in the IGM. The electron fraction \\(X_e(z)\\) depends on ionization states of hydrogen and helium as follows:</p> \\[ X_e(z) = Y_H \\, X_{e,\\mathrm{HII}}(z) + \\frac{1}{4} Y_{\\mathrm{He}} \\left[X_{e,\\mathrm{HeII}}(z) + 2 X_{e,\\mathrm{HeIII}}(z)\\right], \\] <p>where \\(Y_H = 1 - Y_{\\mathrm{He}}\\) and \\(Y_{\\mathrm{He}}\\) are the hydrogen and helium abundances in the Universe, respectively. The ionization fractions \\(X_{e,\\mathrm{H}}(z)\\) and \\(X_{e,\\mathrm{He}}(z)\\) change with redshift and ionization stage.</p> <p>At the epoch of hydrogen reionization, both hydrogen and helium are ionized, so we assume \\(X_{e,\\mathrm{HeII}} = X_{e,\\mathrm{HII}}\\). As hydrogen becomes fully ionized around \\(z = 6\\), we have:</p> <ol> <li>\\(X_{e,\\mathrm{HII}} = 1\\) and \\(X_{e,\\mathrm{HeII}} = 1\\) before helium reionization.</li> <li>When He II reionization occurs around \\(z_{\\mathrm{re}} \\sim 3\\), the ionization state shifts such that \\(X_{e,\\mathrm{HII}} = 1\\), \\(X_{e,\\mathrm{HeII}} = 0\\), and \\(X_{e,\\mathrm{HeIII}} = 1\\).</li> </ol> <p>The electron fraction increases due to He II reionization by approximately \\(7.4\\%\\) because of the transition of He II to He III, as calculated from:</p> \\[ X_e(z &gt; z_{\\mathrm{re}}) = 1 - \\frac{3 Y_{\\mathrm{He}}}{4} \\approx 0.819, \\quad X_e(z &lt; z_{\\mathrm{re}}) = 1 - \\frac{Y_{\\mathrm{He}}}{2} \\approx 0.880. \\] <p>This reionization of He II would cause an observable change in \\(\\mathrm{DM}_{\\mathrm{IGM}}\\), calculated by averaging over different lines of sight:</p> \\[ \\mathrm{DM}_{\\mathrm{IGM}}(z) = \\frac{3 c H_0 \\, \\Omega_b \\, f_{\\mathrm{IGM}}}{8 \\pi G m_p} \\int_0^z \\frac{(1+z') X_e(z')}{\\sqrt{\\Omega_m(1+z')^3 + \\Omega_\\Lambda}} \\, dz'. \\] <p>Simulations show that a significant number of FRBs with well-localized redshifts can yield valuable constraints:</p> <ul> <li>A mock catalog with \\(10^4\\) FRBs can determine the reionization redshift \\(z_{\\mathrm{re}}\\) with an uncertainty of \\(\\sigma(z_{\\mathrm{re}}) \\approx 0.22 - 0.31\\), depending on redshift distribution assumptions.</li> <li>With 1,000 FRBs, the uncertainty on \\(z_{\\mathrm{re}}\\) can reduce to about \\(\\sigma(z_{\\mathrm{re}}) \\approx 0.1\\).</li> </ul> <p>CHORD is forecasted to detect around 10-20 FRBs daily with sub-arcsecond localization, which could make constraining He II reionization realistic.</p>"},{"location":"reasearch/#canadian-hydrogen-observatory-and-radio-transient-detector-chord","title":"Canadian Hydrogen Observatory and Radio-transient Detector (CHORD)","text":"<p>CHORD Commissioning, 2023\u2013Present Supervisors: Matt Dobbs and Dallas Wulf</p> <p>Alongside my work on CHIME, I contribute to the CHORD project, a next-generation radio telescope designed to extend CHIME\u2019s capabilities with enhanced sensitivity, spatial resolution, and frequency range. My role in CHORD focuses on instrumentation and data calibration, specifically analyzing system temperature data for the Deep Dish Development Array (D3A). Working with D3A has provided me with hands-on experience in calibration techniques critical for achieving accurate sky measurements. My responsibilities include assessing thermal noise levels, quantifying environmental effects on system stability, and ensuring consistent data quality across array elements.</p> <p>This detailed, low-level data work has allowed me to develop a comprehensive understanding of radio telescope operation, positioning me to contribute effectively as CHORD moves toward full commissioning. My dual involvement in both CHIME and CHORD has given me a robust foundation in both astrophysical analysis and instrumentation, preparing me to contribute to the future of radio astronomy in Canada and beyond.</p>"},{"location":"reasearch/highlights/awesome_paper/","title":"Awesome Paper","text":"<p>Here is a brief summary of my awesome paper which is published here.  In this paper I postulate if the Inertia of an object Depend Upon Its Energy Content? It is found that:</p> \\[E = mc^2\\] <p>Here is an interesting figure from this paper:</p> <p></p>"},{"location":"reasearch/highlights/large_mwl_paper/","title":"Multiwavelength Paper","text":"<p>In this multiwavelength paper, I use data from VERITAS, Swift and CHIME, to study the equivalencies of sample galaxies when investigating instrumental systematic. Applying Keeler's theorem (see also) it can be shown that convergence is quickly obtained, requiring at minimum two additional sample galaxies.</p> <p> </p> Centered and resized image of the Coma Cluster (source ESA)"}]}